{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d148214-9dfa-44e5-bcb3-955ff119dc8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "This version of TensorFlow Probability requires TensorFlow version >= 2.18; Detected an installation of version 2.16.2. Please upgrade TensorFlow to proceed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Import functions\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     GlobalModel, TrainingModel, NormalisationLayer, \n\u001b[32m     12\u001b[39m     DenormalisationLayer, NormalisedModel, ScaledGlorotUniform\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msampling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinkSample\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeometry\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m form_to_vec, vec_to_form, vec_to_metric\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeometry\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeometry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exterior_derivative, holomorphic_volume_form_to_real\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/G2/Flows/G2Metric/github/sampling/sampling.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Import functions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeometry\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeometry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hermitian_to_kahler_real, holomorphic_volume_form_to_real, compute_gG2\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeometry\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpatches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoordChange_C5R10\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeometry\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwedge_product\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wedge_product\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/G2/Flows/G2Metric/github/geometry/geometry.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m product, permutations\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeometry\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vec_to_form\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeometry\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwedge_product\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wedge_product\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m###########################################################################\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Functions related to the CY-structure\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/G2/Flows/G2Metric/github/geometry/compression.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtfp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m comb, factorial\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m permutations, combinations\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/g2_ml/lib/python3.11/site-packages/tensorflow_probability/__init__.py:22\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Tools for probabilistic reasoning in TensorFlow.\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Contributors to the `python/` dir should not alter this file; instead update\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# `python/__init__.py` as necessary.\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_probability\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# tfp_google.bind(globals())  # DisableOnExport\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# del tfp_google  # DisableOnExport\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/g2_ml/lib/python3.11/site-packages/tensorflow_probability/python/__init__.py:152\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _tf_loaded():\n\u001b[32m    150\u001b[39m   \u001b[38;5;66;03m# Non-lazy load of packages that register with tensorflow or keras.\u001b[39;00m\n\u001b[32m    151\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m pkg_name \u001b[38;5;129;01min\u001b[39;00m _maybe_nonlazy_load:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forces loading the package from its lazy loader.\u001b[39;00m\n\u001b[32m    155\u001b[39m all_util.remove_undocumented(\u001b[34m__name__\u001b[39m, _lazy_load + _maybe_nonlazy_load)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/g2_ml/lib/python3.11/site-packages/tensorflow_probability/python/internal/lazy_loader.py:60\u001b[39m, in \u001b[36mLazyLoader.__dir__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__dir__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m   module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/g2_ml/lib/python3.11/site-packages/tensorflow_probability/python/internal/lazy_loader.py:40\u001b[39m, in \u001b[36mLazyLoader._load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m._on_first_access):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_on_first_access\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m   \u001b[38;5;28mself\u001b[39m._on_first_access = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/g2_ml/lib/python3.11/site-packages/tensorflow_probability/python/__init__.py:59\u001b[39m, in \u001b[36m_validate_tf_environment\u001b[39m\u001b[34m(package)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m#   required_tensorflow_version = '1.15'  # Needed internally -- DisableOnExport\u001b[39;00m\n\u001b[32m     57\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (distutils.version.LooseVersion(tf.__version__) <\n\u001b[32m     58\u001b[39m       distutils.version.LooseVersion(required_tensorflow_version)):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     60\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mThis version of TensorFlow Probability requires TensorFlow \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     61\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mversion >= \u001b[39m\u001b[38;5;132;01m{required}\u001b[39;00m\u001b[33m; Detected an installation of version \u001b[39m\u001b[38;5;132;01m{present}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     62\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mPlease upgrade TensorFlow to proceed.\u001b[39m\u001b[33m'\u001b[39m.format(\n\u001b[32m     63\u001b[39m             required=required_tensorflow_version,\n\u001b[32m     64\u001b[39m             present=tf.__version__))\n\u001b[32m     66\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (package == \u001b[33m'\u001b[39m\u001b[33mmcmc\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m     67\u001b[39m       tf.config.experimental.tensor_float_32_execution_enabled()):\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# Must import here, because symbols get pruned to __all__.\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: This version of TensorFlow Probability requires TensorFlow version >= 2.18; Detected an installation of version 2.16.2. Please upgrade TensorFlow to proceed."
     ]
    }
   ],
   "source": [
    "'''Interactive run file for the G2-structure learning'''\n",
    "#...ensure this notebook is using the correct kernel for your virtual environment\n",
    "# Import libraries\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import functions\n",
    "from models.model import (\n",
    "    GlobalModel, TrainingModel, NormalisationLayer, \n",
    "    DenormalisationLayer, NormalisedModel, ScaledGlorotUniform\n",
    ")\n",
    "from sampling.sampling import LinkSample\n",
    "from geometry.compression import form_to_vec, vec_to_form, vec_to_metric\n",
    "from geometry.geometry import exterior_derivative, holomorphic_volume_form_to_real\n",
    "from geometry.wedge_product import wedge_product\n",
    "from geometry.patches import patch_indices_to_scalar\n",
    "\n",
    "# Print the hyperparameters\n",
    "with open(os.getcwd()+'/hyperparameters/hps.yaml', \"r\") as file:\n",
    "    hp = yaml.safe_load(file)\n",
    "print(f'Run hyperparameters:\\t...edit in hyperparameters/hps.yaml\\n{hp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e724e-9b7e-4c7e-9edf-ce90c0c62c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training script\n",
    "model_name = 'test'\n",
    "!python3 -m run {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d4077-ddfc-49f8-9411-59390b211d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the trained model\n",
    "loaded_model_name = '3form' #model_name #...or set to desired name if not model trained above\n",
    "loaded_model_path = os.getcwd() + f\"/runs/global_model_{loaded_model_name}.keras\" #...reimport the model\n",
    "#loaded_model_path = os.getcwd() + f\"/models/link_models/global_model_3form.keras\" #...import a pre-trained model instead\n",
    "\n",
    "# Custom objects for loading the new architecture\n",
    "custom_objects = {\n",
    "    'GlobalModel': GlobalModel,\n",
    "    'NormalisationLayer': NormalisationLayer,\n",
    "    'DenormalisationLayer': DenormalisationLayer,\n",
    "    'NormalisedModel': NormalisedModel,\n",
    "    'ScaledGlorotUniform': ScaledGlorotUniform\n",
    "}\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model(loaded_model_path, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c48cb-66e6-4596-b583-c09c9d6490aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loaded model on fake link points\n",
    "# Testing hyperparameters\n",
    "testsize = 1000 #...how many link points to use in the testing\n",
    "\n",
    "# Generate the testing data on the Link\n",
    "test_dataset = LinkSample(testsize)\n",
    "test_linkpts = test_dataset.link_points()\n",
    "test_patch_idxs = patch_indices_to_scalar(test_dataset.one_idxs, test_dataset.dropped_idxs)\n",
    "if not hp[\"metric\"]:\n",
    "    test_link_outputs = test_dataset.g2_form\n",
    "else:\n",
    "    test_link_outputs = test_dataset.g2_metric\n",
    "test_linkpts_tf = tf.convert_to_tensor(test_linkpts)\n",
    "test_link_outputs_tf = tf.convert_to_tensor(test_link_outputs)\n",
    "    \n",
    "# Compute the NN test outputs using the new GlobalModel interface\n",
    "if not hp[\"metric\"]:\n",
    "    # Compute the predicted G2 3-forms (loaded_model returns at original scale)\n",
    "    predicted_g2form_vecs = np.array(loaded_model([test_linkpts_tf, test_patch_idxs]))\n",
    "    predicted_g2forms = np.array(vec_to_form(predicted_g2form_vecs, 7, 3).numpy())\n",
    "    print(f'G2 3-forms computed... (shape: {predicted_g2forms.shape})')\n",
    "else:\n",
    "    # Compute the predicted G2 metrics (loaded_model returns at original scale)\n",
    "    predicted_g2metric_vecs = np.array(loaded_model([test_linkpts_tf, test_patch_idxs]))\n",
    "    predicted_g2metrics = np.array(vec_to_metric(predicted_g2metric_vecs, 7, 3).numpy())\n",
    "    print(f'G2 metrics computed... (shape: {predicted_g2metrics.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe010a-57a5-42dc-ba15-8ee88988030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More loss testing\n",
    "test_outputs_vecs = form_to_vec(test_link_outputs_tf)\n",
    "print(test_outputs_vecs.shape, predicted_g2form_vecs.shape)\n",
    "\n",
    "# Mask out near-zero components for evaluation\n",
    "def robust_metrics(y_true, y_pred, threshold=0.0001):\n",
    "    mask = np.abs(y_true) > threshold\n",
    "    if np.sum(mask) > 0:\n",
    "        active_true = y_true[mask]\n",
    "        active_pred = y_pred[mask]\n",
    "        \n",
    "        mae_active = np.mean(np.abs(active_true - active_pred))\n",
    "        mape_active = 100 * np.mean(np.abs(active_true - active_pred) / np.abs(active_true))\n",
    "        \n",
    "        return mae_active, mape_active, np.sum(mask)\n",
    "    return None, None, 0\n",
    "\n",
    "# Define losses\n",
    "mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "mape = tf.keras.losses.MeanAbsolutePercentageError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "\n",
    "# Compute losses\n",
    "mae_loss = mae(test_outputs_vecs, predicted_g2form_vecs).numpy()\n",
    "mse_loss = mse(test_outputs_vecs, predicted_g2form_vecs).numpy()\n",
    "mape_loss = mape(test_outputs_vecs, predicted_g2form_vecs).numpy()\n",
    "mae_active, mape_active, n_active = robust_metrics(\n",
    "    test_outputs_vecs.numpy().flatten(), \n",
    "    predicted_g2form_vecs.flatten()\n",
    ")\n",
    "\n",
    "# Print losses\n",
    "print(\"(MAE, MSE, MAPE):\", (mae_loss, mse_loss, mape_loss))\n",
    "print(f\"Active components: {n_active}/{np.prod(test_outputs_vecs.shape)}\")\n",
    "print(f\"Active MAE: {mae_active:.6f}\")\n",
    "print(f\"Active MAPE: {mape_active:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04483165-770d-4375-b280-e6ad0944435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.absolute(predicted_g2form_vecs),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this analysis in your notebook:\n",
    "print(\"Target value statistics:\")\n",
    "print(f\"Min: {np.min(test_outputs_vecs):.6f}\")\n",
    "print(f\"Max: {np.max(test_outputs_vecs):.6f}\")\n",
    "print(f\"Mean: {np.mean(test_outputs_vecs):.6f}\")\n",
    "print(f\"Values near zero: {np.sum(np.abs(test_outputs_vecs) < 0.01)}/{np.prod(test_outputs_vecs.shape)}\")\n",
    "\n",
    "# Check component-wise errors\n",
    "component_mapes = 100 * np.mean(np.abs(test_outputs_vecs - predicted_g2form_vecs) / \n",
    "                               (np.abs(test_outputs_vecs) + 1e-8), axis=0)\n",
    "print(f\"Per-component MAPE: {component_mapes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f413da-67e3-42ea-ab2f-df7596f65912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "component_abs_means = np.mean(np.absolute(test_outputs_vecs), axis=0)\n",
    "print(component_abs_means)\n",
    "print(np.where(component_abs_means == 0))\n",
    "\n",
    "#Abs(Mean)\n",
    "plt.figure()\n",
    "plt.bar(range(len(component_abs_means)), component_abs_means)\n",
    "plt.xlabel('component')\n",
    "plt.ylabel('abs(mean)')\n",
    "plt.grid()\n",
    "plt.yscale('log')\n",
    "\n",
    "#MAPE\n",
    "plt.figure()\n",
    "plt.bar(range(len(component_mapes)), component_mapes)\n",
    "plt.xlabel('component')\n",
    "plt.ylabel('mape')\n",
    "plt.grid()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102e1b5-4cfe-4486-b8e4-7d269144b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute exterior derivative of 3-forms over the test data\n",
    "# Retrieve the Kahler form for the test dataset\n",
    "test_kahler_form = test_dataset.kahler_form\n",
    "\n",
    "# Compute d\\phi\n",
    "dg2_3form = np.array(exterior_derivative(loaded_model, test_linkpts_tf, test_patch_idxs))\n",
    "print(f'G2 3-form exterior derivatives computed... (shape: {dg2_3form.shape})')\n",
    "###print(f'Non-zero elements: {np.sum(np.where(np.absolute(np.mean(dg2_3form[0],axis=0)) < 1e-5, 1, 0))} / {np.prod(dg2_3form.shape[2:])}')\n",
    "\n",
    "# Compute omega ^ omega\n",
    "omega_wedge_omega = np.array([wedge_product(test_kahler_form[idx], test_kahler_form[idx]) for idx in range(test_kahler_form.shape[0])])\n",
    "print(f'\\omega ^ \\omega computed... (shape: {omega_wedge_omega.shape})')\n",
    "\n",
    "# Check whether dg2_3form == omega_wedge_omega\n",
    "tolerance = 1e3-6\n",
    "print(f'Checking identity d\\phi = \\omega ^ \\omega:\\t{np.allclose(dg2_3form, omega_wedge_omega)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57511a7c-2d62-4cb1-9bae-022b9c8abe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute exterior derivative of 3-forms over the test data\n",
    "# Retrieve the Kahler form for the test dataset\n",
    "test_kahler_form = test_dataset.kahler_form\n",
    "\n",
    "# Compute d\\phi - use the GlobalModel directly (no .model attribute needed)\n",
    "dg2_3form = np.array(exterior_derivative(loaded_model, test_linkpts_tf, test_patch_idxs))\n",
    "print(f'G2 3-form exterior derivatives computed... (shape: {dg2_3form.shape})')\n",
    "###print(f'Non-zero elements: {np.sum(np.where(np.absolute(np.mean(dg2_3form[0],axis=0)) < 1e-5, 1, 0))} / {np.prod(dg2_3form.shape[2:])}')\n",
    "\n",
    "# Compute omega ^ omega\n",
    "omega_wedge_omega = np.array([wedge_product(test_kahler_form[idx], test_kahler_form[idx]) for idx in range(test_kahler_form.shape[0])])\n",
    "print(f'\\omega ^ \\omega computed... (shape: {omega_wedge_omega.shape})')\n",
    "\n",
    "# Check whether dg2_3form == omega_wedge_omega\n",
    "tolerance = 1e3-6\n",
    "print(f'Checking identity d\\phi = \\omega ^ \\omega:\\t{np.allclose(dg2_3form, omega_wedge_omega)}')\n",
    "im_hvf_wedge_dtheta = np.array([wedge_product(test_hvf_i_R7[i], test_dataset.dthetas[i]) for i in range(test_hvf_i_R7.shape[0])])\n",
    "# Full \\psi\n",
    "psi_v2 = 0.5 * omega_wedge_omega + im_hvf_wedge_dtheta\n",
    "\n",
    "print(psi_v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731da13-8db4-4dca-aa27-e14b59aacb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "### IGNORE BELOW ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcce8a-86d3-4be0-8041-abf84efd8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###manual checking -- deleteee\n",
    "#np.sum(np.where(np.absolute(omega_wedge_omega) > 1e-8, 1, 0)),np.sum(np.where(np.absolute(dg2_3form) > 1e-8, 1, 0))\n",
    "#print(np.min(dg2_3form), np.mean(dg2_3form), np.max(dg2_3form))\n",
    "#print(np.min(omega_wedge_omega), np.mean(omega_wedge_omega), np.max(omega_wedge_omega))\n",
    "\n",
    "'''\n",
    "for pt_idx in range(identity_test_size):\n",
    "    print(np.mean(np.absolute(dg2_3form[0][pt_idx] - omega_wedge_omega[pt_idx])))\n",
    "    ###reduce to non-zero?\n",
    "    #--> consistently baddd\n",
    "'''\n",
    "\n",
    "print(np.mean(test_linkpts, axis=0), np.std(test_linkpts, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b20c86-ec69-4d7b-998c-71ddcf7b3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "###testinggg\n",
    "from geometry.geometry import wedge_product, wedge_form2_with_form1\n",
    "fake_batchsize = 10\n",
    "'''#tf\n",
    "fake_2_forms = tf.random.normal((fake_batchsize, 6, 6))\n",
    "fake_2_forms = fake_2_forms - tf.transpose(fake_2_forms, perm=[0, 2, 1])\n",
    "fake_1_forms = tf.random.normal((fake_batchsize, 7))\n",
    "'''\n",
    "#np\n",
    "fake_2_forms = np.random.randn(fake_batchsize, 6, 6)\n",
    "fake_2_forms_66 = fake_2_forms - np.transpose(fake_2_forms, axes=[0, 2, 1])\n",
    "fake_2_forms_77 = np.pad(fake_2_forms_66, ((0,0), (0,1), (0,1)), mode='constant')\n",
    "fake_1_forms = np.random.normal(size=(fake_batchsize, 7))\n",
    "print(f'Data shapes: {fake_2_forms.shape}, {fake_1_forms.shape}')\n",
    "\n",
    "# old functionality:\n",
    "output_old = wedge_form2_with_form1(fake_2_forms_66, fake_1_forms)\n",
    "output_new = np.array([wedge_product(fake_2_forms_77[idx], fake_1_forms[idx]) for idx in range(fake_1_forms.shape[0])])\n",
    "\n",
    "# scale output_old to match output_new\n",
    "output_old *= 3 ###why is there this factor of 3 difference?\n",
    "\n",
    "print(f'Output shapes: {output_old.shape}, {output_new.shape}')\n",
    "print(f'Matching?? --> {np.allclose(output_old, output_new)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcaeacb-4ec3-4850-be5d-bac62305cd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (g2_ml Conda)",
   "language": "python",
   "name": "g2_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
