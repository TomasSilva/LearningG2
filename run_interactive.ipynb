{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d148214-9dfa-44e5-bcb3-955ff119dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run hyperparameters:\t...edit in hyperparameters/hps.yaml\n",
      "{'metric': False, 'num_samples': 200000, 'saved_model': False, 'saved_model_path': '...', 'n_hidden': 128, 'n_layers': 4, 'activations': 'gelu', 'use_bias': True, 'parameter_initialisation_scale': 1.0, 'embedding_dim': 8, 'epochs': 200, 'batch_size': 32, 'init_learning_rate': 0.001, 'min_learning_rate': 0.0001, 'validate': True, 'val_print': False, 'num_val_samples': 500, 'val_batch_size': 100, 'verbosity': 1, 'print_losses': False, 'print_interval': 1}\n"
     ]
    }
   ],
   "source": [
    "'''Interactive run file for the G2-structure learning'''\n",
    "#...ensure this notebook is using the correct kernel for your virtual environment\n",
    "# Import libraries\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import functions\n",
    "from models.model import (\n",
    "    GlobalModel, TrainingModel, NormalisationLayer, \n",
    "    DenormalisationLayer, NormalisedModel, ScaledGlorotUniform\n",
    ")\n",
    "from sampling.sampling import LinkSample\n",
    "from geometry.compression import form_to_vec, vec_to_form, vec_to_metric\n",
    "from geometry.geometry import exterior_derivative, holomorphic_volume_form_to_real\n",
    "from geometry.wedge_product import wedge_product\n",
    "from geometry.patches import patch_indices_to_scalar\n",
    "\n",
    "# Print the hyperparameters\n",
    "with open(os.getcwd()+'/hyperparameters/hps.yaml', \"r\") as file:\n",
    "    hp = yaml.safe_load(file)\n",
    "print(f'Run hyperparameters:\\t...edit in hyperparameters/hps.yaml\\n{hp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e724e-9b7e-4c7e-9edf-ce90c0c62c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training script\n",
    "model_name = 'test'\n",
    "!python3 -m run {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31d4077-ddfc-49f8-9411-59390b211d70",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MockHP' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m      7\u001b[0m custom_objects \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGlobalModel\u001b[39m\u001b[38;5;124m'\u001b[39m: GlobalModel,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormalisationLayer\u001b[39m\u001b[38;5;124m'\u001b[39m: NormalisationLayer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScaledGlorotUniform\u001b[39m\u001b[38;5;124m'\u001b[39m: ScaledGlorotUniform\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cymetric/lib/python3.9/site-packages/keras/src/saving/saving_api.py:254\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following argument(s) are not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the native Keras format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         )\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    263\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    264\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/cymetric/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:281\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    278\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniforge3/envs/cymetric/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:246\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 246\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
      "File \u001b[0;32m~/miniforge3/envs/cymetric/lib/python3.9/site-packages/keras/src/saving/serialization_lib.py:728\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m safe_mode_scope \u001b[38;5;241m=\u001b[39m SafeModeScope(safe_mode)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[0;32m--> 728\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m     build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m build_config:\n",
      "File \u001b[0;32m~/Documents/Projects/G2/Flows/G2Metric/github/models/model.py:290\u001b[0m, in \u001b[0;36mGlobalModel.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(hp, key, value)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Create the model\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m model\u001b[38;5;241m.\u001b[39m_normalisation_fitted \u001b[38;5;241m=\u001b[39m normalisation_fitted\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# If normalisation was fitted, build the model\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/G2/Flows/G2Metric/github/models/model.py:185\u001b[0m, in \u001b[0;36mGlobalModel.__init__\u001b[0;34m(self, hp, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp \u001b[38;5;241m=\u001b[39m hp\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserializable_hp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_serializable_hp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Create normalisation layers\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_normaliser \u001b[38;5;241m=\u001b[39m NormalisationLayer(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_normaliser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/G2/Flows/G2Metric/github/models/model.py:262\u001b[0m, in \u001b[0;36mGlobalModel.set_serializable_hp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_serializable_hp\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserializable_hp \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 262\u001b[0m         key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_serializable(value)\n\u001b[1;32m    263\u001b[0m     }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MockHP' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# Import the trained model\n",
    "loaded_model_name = '3form' #model_name #...or set to desired name if not model trained above\n",
    "loaded_model_path = os.getcwd() + f\"/runs/global_model_{loaded_model_name}.keras\" #...reimport the model\n",
    "#loaded_model_path = os.getcwd() + f\"/models/link_models/global_model_3form.keras\" #...import a pre-trained model instead\n",
    "\n",
    "# Custom objects for loading the new architecture\n",
    "custom_objects = {\n",
    "    'GlobalModel': GlobalModel,\n",
    "    'NormalisationLayer': NormalisationLayer,\n",
    "    'DenormalisationLayer': DenormalisationLayer,\n",
    "    'NormalisedModel': NormalisedModel,\n",
    "    'ScaledGlorotUniform': ScaledGlorotUniform\n",
    "}\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model(loaded_model_path, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c48cb-66e6-4596-b583-c09c9d6490aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loaded model on fake link points\n",
    "# Testing hyperparameters\n",
    "testsize = 100 #...how many link points to use in the testing\n",
    "\n",
    "# Generate the testing data on the Link\n",
    "test_dataset = LinkSample(testsize)\n",
    "test_linkpts = test_dataset.link_points()\n",
    "test_patch_idxs = patch_indices_to_scalar(test_dataset.one_idxs, test_dataset.dropped_idxs)\n",
    "if not hp[\"metric\"]:\n",
    "    test_link_outputs = test_dataset.g2_form\n",
    "else:\n",
    "    test_link_outputs = test_dataset.g2_metric\n",
    "test_linkpts_tf = tf.convert_to_tensor(test_linkpts)\n",
    "test_link_outputs_tf = tf.convert_to_tensor(test_link_outputs)\n",
    "    \n",
    "# Compute the NN test outputs using the new GlobalModel interface\n",
    "if not hp[\"metric\"]:\n",
    "    # Compute the predicted G2 3-forms (loaded_model returns at original scale)\n",
    "    predicted_g2form_vecs = np.array(loaded_model([test_linkpts_tf, test_patch_idxs]))\n",
    "    predicted_g2forms = np.array(vec_to_form(predicted_g2form_vecs, 7, 3).numpy())\n",
    "    print(f'G2 3-forms computed... (shape: {predicted_g2forms.shape})')\n",
    "else:\n",
    "    # Compute the predicted G2 metrics (loaded_model returns at original scale)\n",
    "    predicted_g2metric_vecs = np.array(loaded_model([test_linkpts_tf, test_patch_idxs]))\n",
    "    predicted_g2metrics = np.array(vec_to_metric(predicted_g2metric_vecs, 7, 3).numpy())\n",
    "    print(f'G2 metrics computed... (shape: {predicted_g2metrics.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe010a-57a5-42dc-ba15-8ee88988030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More loss testing\n",
    "print(test_outputs_vecs.shape, predicted_g2form_vecs.shape)\n",
    "\n",
    "# Define losses\n",
    "mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "mape = tf.keras.losses.MeanAbsolutePercentageError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "\n",
    "# Compute losses\n",
    "mae_loss = mae(test_outputs_vecs, predicted_g2form_vecs).numpy()\n",
    "mse_loss = mse(test_outputs_vecs, predicted_g2form_vecs).numpy()\n",
    "mape_loss = mape(test_outputs_vecs, predicted_g2form_vecs).numpy()\n",
    "\n",
    "# Print losses\n",
    "print(\"(MAE, MSE, MAPE):\", (mae_loss, mse_loss, mape_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102e1b5-4cfe-4486-b8e4-7d269144b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute exterior derivative of 3-forms over the test data\n",
    "# Retrieve the Kahler form for the test dataset\n",
    "test_kahler_form = test_dataset.kahler_form\n",
    "\n",
    "# Compute d\\phi\n",
    "dg2_3form = np.array(exterior_derivative(loaded_model.model, test_linkpts_tf, test_patch_idxs))\n",
    "print(f'G2 3-form exterior derivatives computed... (shape: {dg2_3form.shape})')\n",
    "###print(f'Non-zero elements: {np.sum(np.where(np.absolute(np.mean(dg2_3form[0],axis=0)) < 1e-5, 1, 0))} / {np.prod(dg2_3form.shape[2:])}')\n",
    "\n",
    "# Compute omega ^ omega\n",
    "omega_wedge_omega = np.array([wedge_product(test_kahler_form[idx], test_kahler_form[idx]) for idx in range(test_kahler_form.shape[0])])\n",
    "print(f'\\omega ^ \\omega computed... (shape: {omega_wedge_omega.shape})')\n",
    "\n",
    "# Check whether dg2_3form == omega_wedge_omega\n",
    "tolerance = 1e3-6\n",
    "print(f'Checking identity d\\phi = \\omega ^ \\omega:\\t{np.allclose(dg2_3form, omega_wedge_omega)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57511a7c-2d62-4cb1-9bae-022b9c8abe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute \\psi both ways ---> need the metric working first\n",
    "# Compute as *\\phi\n",
    "from geometry.numerical_star_R7 import Hodge_Dual\n",
    "\n",
    "##train 'good' model for metric\n",
    "##compute metric & 3-form on test data\n",
    "#psi_v1 = np.array([Hodge_Dual(predicted_g2forms[pt_idx], predicted_g2metrics[pt_idx]) for pt_idx in range(testsize)])\n",
    "\n",
    "# Compute as 0.5 * \\omega ^ \\omega + Im(\\Omega) ^ d\\theta\n",
    "# 1st term\n",
    "if 'omega_wedge_omega' not in locals():\n",
    "    kf = test_dataset.kahler_form\n",
    "    omega_wedge_omega = np.array([wedge_product(kf[idx], kf[idx]) for idx in range(kf.shape[0])])\n",
    "# 2nd term\n",
    "test_hvf_r, test_hvf_i = holomorphic_volume_form_to_real(test_dataset.holomorphic_volume_form)\n",
    "test_hvf_i_R7 = np.pad(test_hvf_i, ((0,0), (0,1), (0,1), (0,1)), mode='constant')\n",
    "im_hvf_wedge_dtheta = np.array([wedge_product(test_hvf_i_R7[i], test_dataset.dthetas[i]) for i in range(test_hvf_i_R7.shape[0])])\n",
    "# Full \\psi\n",
    "psi_v2 = 0.5 * omega_wedge_omega + im_hvf_wedge_dtheta\n",
    "\n",
    "print(psi_v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731da13-8db4-4dca-aa27-e14b59aacb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "### IGNORE BELOW ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcce8a-86d3-4be0-8041-abf84efd8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###manual checking -- deleteee\n",
    "#np.sum(np.where(np.absolute(omega_wedge_omega) > 1e-8, 1, 0)),np.sum(np.where(np.absolute(dg2_3form) > 1e-8, 1, 0))\n",
    "#print(np.min(dg2_3form), np.mean(dg2_3form), np.max(dg2_3form))\n",
    "#print(np.min(omega_wedge_omega), np.mean(omega_wedge_omega), np.max(omega_wedge_omega))\n",
    "\n",
    "'''\n",
    "for pt_idx in range(identity_test_size):\n",
    "    print(np.mean(np.absolute(dg2_3form[0][pt_idx] - omega_wedge_omega[pt_idx])))\n",
    "    ###reduce to non-zero?\n",
    "    #--> consistently baddd\n",
    "'''\n",
    "\n",
    "print(np.mean(test_linkpts, axis=0), np.std(test_linkpts, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b20c86-ec69-4d7b-998c-71ddcf7b3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "###testinggg\n",
    "from geometry.geometry import wedge_product, wedge_form2_with_form1\n",
    "fake_batchsize = 10\n",
    "'''#tf\n",
    "fake_2_forms = tf.random.normal((fake_batchsize, 6, 6))\n",
    "fake_2_forms = fake_2_forms - tf.transpose(fake_2_forms, perm=[0, 2, 1])\n",
    "fake_1_forms = tf.random.normal((fake_batchsize, 7))\n",
    "'''\n",
    "#np\n",
    "fake_2_forms = np.random.randn(fake_batchsize, 6, 6)\n",
    "fake_2_forms_66 = fake_2_forms - np.transpose(fake_2_forms, axes=[0, 2, 1])\n",
    "fake_2_forms_77 = np.pad(fake_2_forms_66, ((0,0), (0,1), (0,1)), mode='constant')\n",
    "fake_1_forms = np.random.normal(size=(fake_batchsize, 7))\n",
    "print(f'Data shapes: {fake_2_forms.shape}, {fake_1_forms.shape}')\n",
    "\n",
    "# old functionality:\n",
    "output_old = wedge_form2_with_form1(fake_2_forms_66, fake_1_forms)\n",
    "output_new = np.array([wedge_product(fake_2_forms_77[idx], fake_1_forms[idx]) for idx in range(fake_1_forms.shape[0])])\n",
    "\n",
    "# scale output_old to match output_new\n",
    "output_old *= 3 ###why is there this factor of 3 difference?\n",
    "\n",
    "print(f'Output shapes: {output_old.shape}, {output_new.shape}')\n",
    "print(f'Matching?? --> {np.allclose(output_old, output_new)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcaeacb-4ec3-4850-be5d-bac62305cd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cymetric",
   "language": "python",
   "name": "cymetric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
