{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d148214-9dfa-44e5-bcb3-955ff119dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Interactive run file for the G2-structure learning'''\n",
    "#...ensure this notebook is using the correct kernel for your virtual environment\n",
    "# Import libraries\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import functions\n",
    "from models.model import (\n",
    "    GlobalModel, TrainingModel, NormalisationLayer, \n",
    "    DenormalisationLayer, NormalisedModel, ScaledGlorotUniform\n",
    ")\n",
    "from sampling.sampling import LinkSample\n",
    "from geometry.compression import form_to_vec, vec_to_form, vec_to_metric\n",
    "from geometry.geometry import exterior_derivative, holomorphic_volume_form_to_real\n",
    "from geometry.wedge_product import wedge_product\n",
    "from geometry.patches import patch_indices_to_scalar\n",
    "\n",
    "# Print the hyperparameters\n",
    "with open(os.getcwd()+'/hyperparameters/hps.yaml', \"r\") as file:\n",
    "    hp = yaml.safe_load(file)\n",
    "print(f'Run hyperparameters:\\t...edit in hyperparameters/hps.yaml\\n{hp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e724e-9b7e-4c7e-9edf-ce90c0c62c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training script\n",
    "model_name = 'test'\n",
    "!python3 -m run {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d4077-ddfc-49f8-9411-59390b211d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the trained model\n",
    "loaded_model_name = model_name #...or set to desired name if not model trained above\n",
    "loaded_model_path = os.getcwd() + f\"/runs/global_model_{loaded_model_name}.keras\" #...reimport the model\n",
    "#loaded_model_path = os.getcwd() + f\"/models/link_models/link_model_3form.keras\" #...import a pre-trained model instead\n",
    "\n",
    "# Custom objects for loading the new architecture\n",
    "custom_objects = {\n",
    "    'GlobalModel': GlobalModel,\n",
    "    'NormalisationLayer': NormalisationLayer,\n",
    "    'DenormalisationLayer': DenormalisationLayer,\n",
    "    'NormalisedModel': NormalisedModel,\n",
    "    'ScaledGlorotUniform': ScaledGlorotUniform\n",
    "}\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model(loaded_model_path, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c48cb-66e6-4596-b583-c09c9d6490aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loaded model on fake link points\n",
    "# Testing hyperparameters\n",
    "testsize = 100 #...how many link points to use in the testing\n",
    "\n",
    "# Generate the testing data on the Link\n",
    "test_dataset = LinkSample(testsize)\n",
    "test_linkpts = test_dataset.link_points()\n",
    "test_patch_idxs = patch_indices_to_scalar(test_dataset.one_idxs, test_dataset.dropped_idxs)\n",
    "if not hp[\"metric\"]:\n",
    "    test_link_outputs = test_dataset.g2_form\n",
    "else:\n",
    "    test_link_outputs = test_dataset.g2_metric\n",
    "test_linkpts_tf = tf.convert_to_tensor(test_linkpts)\n",
    "test_link_outputs_tf = tf.convert_to_tensor(test_link_outputs)\n",
    "    \n",
    "# Compute the NN test outputs using the new GlobalModel interface\n",
    "if not hp[\"metric\"]:\n",
    "    # Compute the predicted G2 3-forms (loaded_model returns at original scale)\n",
    "    predicted_g2form_vecs = np.array(loaded_model([test_linkpts_tf, test_patch_idxs]))\n",
    "    predicted_g2forms = np.array(vec_to_form(predicted_g2form_vecs, 7, 3).numpy())\n",
    "    print(f'G2 3-forms computed... (shape: {predicted_g2forms.shape})')\n",
    "else:\n",
    "    # Compute the predicted G2 metrics (loaded_model returns at original scale)\n",
    "    predicted_g2metric_vecs = np.array(loaded_model([test_linkpts_tf, test_patch_idxs]))\n",
    "    predicted_g2metrics = np.array(vec_to_metric(predicted_g2metric_vecs, 7, 3).numpy())\n",
    "    print(f'G2 metrics computed... (shape: {predicted_g2metrics.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe010a-57a5-42dc-ba15-8ee88988030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More loss testing\n",
    "print(test_outputs_vecs.shape, predicted_g2form_vecs.shape)\n",
    "\n",
    "# Define losses\n",
    "mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "mape = tf.keras.losses.MeanAbsolutePercentageError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "\n",
    "# Compute losses\n",
    "mae_loss = mae(test_outputs_vecs, predicted_g2form_vecs).numpy()\n",
    "mse_loss = mse(test_outputs_vecs, predicted_g2form_vecs).numpy()\n",
    "mape_loss = mape(test_outputs_vecs, predicted_g2form_vecs).numpy()\n",
    "\n",
    "# Print losses\n",
    "print(\"(MAE, MSE, MAPE):\", (mae_loss, mse_loss, mape_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102e1b5-4cfe-4486-b8e4-7d269144b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute exterior derivative of 3-forms over the test data\n",
    "# Retrieve the Kahler form for the test dataset\n",
    "test_kahler_form = test_dataset.kahler_form\n",
    "\n",
    "# Compute d\\phi\n",
    "dg2_3form = np.array(exterior_derivative(loaded_model.model, test_linkpts_tf, test_patch_idxs))\n",
    "print(f'G2 3-form exterior derivatives computed... (shape: {dg2_3form.shape})')\n",
    "###print(f'Non-zero elements: {np.sum(np.where(np.absolute(np.mean(dg2_3form[0],axis=0)) < 1e-5, 1, 0))} / {np.prod(dg2_3form.shape[2:])}')\n",
    "\n",
    "# Compute omega ^ omega\n",
    "omega_wedge_omega = np.array([wedge_product(test_kahler_form[idx], test_kahler_form[idx]) for idx in range(test_kahler_form.shape[0])])\n",
    "print(f'\\omega ^ \\omega computed... (shape: {omega_wedge_omega.shape})')\n",
    "\n",
    "# Check whether dg2_3form == omega_wedge_omega\n",
    "tolerance = 1e3-6\n",
    "print(f'Checking identity d\\phi = \\omega ^ \\omega:\\t{np.allclose(dg2_3form, omega_wedge_omega)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57511a7c-2d62-4cb1-9bae-022b9c8abe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute \\psi both ways ---> need the metric working first\n",
    "# Compute as *\\phi\n",
    "from geometry.numerical_star_R7 import Hodge_Dual\n",
    "\n",
    "##train 'good' model for metric\n",
    "##compute metric & 3-form on test data\n",
    "#psi_v1 = np.array([Hodge_Dual(predicted_g2forms[pt_idx], predicted_g2metrics[pt_idx]) for pt_idx in range(testsize)])\n",
    "\n",
    "# Compute as 0.5 * \\omega ^ \\omega + Im(\\Omega) ^ d\\theta\n",
    "# 1st term\n",
    "if 'omega_wedge_omega' not in locals():\n",
    "    kf = test_dataset.kahler_form\n",
    "    omega_wedge_omega = np.array([wedge_product(kf[idx], kf[idx]) for idx in range(kf.shape[0])])\n",
    "# 2nd term\n",
    "test_hvf_r, test_hvf_i = holomorphic_volume_form_to_real(test_dataset.holomorphic_volume_form)\n",
    "test_hvf_i_R7 = np.pad(test_hvf_i, ((0,0), (0,1), (0,1), (0,1)), mode='constant')\n",
    "im_hvf_wedge_dtheta = np.array([wedge_product(test_hvf_i_R7[i], test_dataset.dthetas[i]) for i in range(test_hvf_i_R7.shape[0])])\n",
    "# Full \\psi\n",
    "psi_v2 = 0.5 * omega_wedge_omega + im_hvf_wedge_dtheta\n",
    "\n",
    "print(psi_v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731da13-8db4-4dca-aa27-e14b59aacb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "### IGNORE BELOW ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcce8a-86d3-4be0-8041-abf84efd8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###manual checking -- deleteee\n",
    "#np.sum(np.where(np.absolute(omega_wedge_omega) > 1e-8, 1, 0)),np.sum(np.where(np.absolute(dg2_3form) > 1e-8, 1, 0))\n",
    "#print(np.min(dg2_3form), np.mean(dg2_3form), np.max(dg2_3form))\n",
    "#print(np.min(omega_wedge_omega), np.mean(omega_wedge_omega), np.max(omega_wedge_omega))\n",
    "\n",
    "'''\n",
    "for pt_idx in range(identity_test_size):\n",
    "    print(np.mean(np.absolute(dg2_3form[0][pt_idx] - omega_wedge_omega[pt_idx])))\n",
    "    ###reduce to non-zero?\n",
    "    #--> consistently baddd\n",
    "'''\n",
    "\n",
    "print(np.mean(test_linkpts, axis=0), np.std(test_linkpts, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b20c86-ec69-4d7b-998c-71ddcf7b3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "###testinggg\n",
    "from geometry.geometry import wedge_product, wedge_form2_with_form1\n",
    "fake_batchsize = 10\n",
    "'''#tf\n",
    "fake_2_forms = tf.random.normal((fake_batchsize, 6, 6))\n",
    "fake_2_forms = fake_2_forms - tf.transpose(fake_2_forms, perm=[0, 2, 1])\n",
    "fake_1_forms = tf.random.normal((fake_batchsize, 7))\n",
    "'''\n",
    "#np\n",
    "fake_2_forms = np.random.randn(fake_batchsize, 6, 6)\n",
    "fake_2_forms_66 = fake_2_forms - np.transpose(fake_2_forms, axes=[0, 2, 1])\n",
    "fake_2_forms_77 = np.pad(fake_2_forms_66, ((0,0), (0,1), (0,1)), mode='constant')\n",
    "fake_1_forms = np.random.normal(size=(fake_batchsize, 7))\n",
    "print(f'Data shapes: {fake_2_forms.shape}, {fake_1_forms.shape}')\n",
    "\n",
    "# old functionality:\n",
    "output_old = wedge_form2_with_form1(fake_2_forms_66, fake_1_forms)\n",
    "output_new = np.array([wedge_product(fake_2_forms_77[idx], fake_1_forms[idx]) for idx in range(fake_1_forms.shape[0])])\n",
    "\n",
    "# scale output_old to match output_new\n",
    "output_old *= 3 ###why is there this factor of 3 difference?\n",
    "\n",
    "print(f'Output shapes: {output_old.shape}, {output_new.shape}')\n",
    "print(f'Matching?? --> {np.allclose(output_old, output_new)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcaeacb-4ec3-4850-be5d-bac62305cd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cymetric",
   "language": "python",
   "name": "cymetric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
