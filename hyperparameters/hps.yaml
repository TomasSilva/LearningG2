### Hyperparameters for the G2-structure learning runs ###

###############################################################################
# Geometric set-up
# Select whether to learn the G2-metric (true) or 3-form (false)
metric: false

# Specify which CY metric run to use for G2 sampling
# null = use most recent CY model, int = use specific run number
cymetric_run_number: null

###############################################################################
# Data set-up
# Number of training samples to use (null = use all, int = random subset from g2_train.npz)
num_samples: null

# Number of validation samples to use (null = use all, int = random subset from g2_val.npz)
num_val_samples: null

###############################################################################
# Model set-up
# Path to a saved model to load and continue training (null means train from scratch)
saved_model_path: null

# Number of hidden units in each layer
n_hidden: 256

# Number of layers in the neural network
n_layers: 4

# Activation function to use in the neural network (e.g., gelu, relu)
activations: gelu

# Whether to use bias terms in the neural network layers
use_bias: true

# Scale for parameter initialization (uses uniform distribution [-scale, scale])
parameter_initialisation_scale: 1.0

# Normalization settings
normalize_inputs: true      # Whether to normalize inputs (recommended: true for stable training)
normalize_outputs: true     # Whether to normalize outputs (recommended: true for stable training)

# Huber loss parameter (set to null or inf to use standard MSE loss)
# Recommended: null for MSE, 1.0 for robust training with outliers
huber_delta: null

# Regularization parameters
dropout_rate: 0.0           # Dropout rate (0.0 = no dropout, 0.1-0.3 typical)
l2_regularization: 0.0      # L2 weight decay (0.0 = none, 1e-5 to 1e-3 typical)

###############################################################################
# Training set-up
# Number of training epochs
epochs: 300

# Batch size for training
batch_size: 512

# Learning rate for the optimizer
init_learning_rate: 0.001

# Learning rate scheduler settings (ReduceLROnPlateau)
lr_reduce_factor: 0.5       # Factor to reduce LR (new_lr = lr * factor)
lr_reduce_patience: 8       # Number of epochs with no improvement before reducing LR
min_learning_rate: 1.0e-6   # Minimum learning rate

# Early stopping settings
early_stop_patience: 20     # Number of epochs with no improvement before stopping

# Validation settings (uses samples/link_data/g2_val.npz file)
validate: true              # Whether to use validation during training
val_batch_size: 512         # Batch size for validation

# Verbosity level: 0 = silent, 1 = progress bar, 2 = one line per epoch
verbosity: 1
