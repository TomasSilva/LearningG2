{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362b44a2",
   "metadata": {},
   "source": [
    "# Learning Holomorphic Volume Form with Neural Network\n",
    "\n",
    "This notebook trains a feedforward neural network to learn the holomorphic volume form (Omega) from CY point coordinates.\n",
    "\n",
    "**Goal:** Learn the real and imaginary parts of the complex holomorphic volume form components.\n",
    "\n",
    "**Steps:**\n",
    "1. Setup and generate CY points\n",
    "2. Compute holomorphic volume form and Hermitian metric\n",
    "3. Prepare training data\n",
    "4. Train neural network\n",
    "5. Test on independent sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2edb8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b29268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup path for cymetric package (from scratch/ to github/)\n",
    "_parent_dir = pathlib.Path(os.getcwd()).parent\n",
    "_cymetric_dir = _parent_dir.parent / \"cymetric\"\n",
    "if str(_parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(_parent_dir))\n",
    "if str(_cymetric_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(_cymetric_dir))\n",
    "\n",
    "# Create alias to fix cymetric internal imports\n",
    "import cymetric\n",
    "if hasattr(cymetric, 'cymetric'):\n",
    "    sys.modules['cymetric'] = cymetric.cymetric\n",
    "\n",
    "# Import geometry functions\n",
    "from geometry.patches import CoordChange_C5R10\n",
    "\n",
    "# Import cymetric functions\n",
    "from cymetric.pointgen.pointgen import PointGenerator\n",
    "from cymetric.models.tfhelper import prepare_tf_basis\n",
    "from cymetric.models.tfmodels import PhiFSModel\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45d4bc",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b55c16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Monomials shape: (5, 5)\n",
      "  Ambient dimension: [4]\n"
     ]
    }
   ],
   "source": [
    "# Set up paths (go to parent directory)\n",
    "cymodel_name = ''  # Set to '_name' if using named model\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "dirname = os.path.join(parent_dir, 'models', 'cy_models', 'link_data')\n",
    "config_path = os.path.join(parent_dir, f'models/cy_models/cy_model_config{cymodel_name}.yaml')\n",
    "cymodel_path = os.path.join(parent_dir, f'models/cy_models/cy_metric_model{cymodel_name}.keras')\n",
    "\n",
    "# Load configuration\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.unsafe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Monomials shape: {np.array(config['monomials']).shape}\")\n",
    "print(f\"  Ambient dimension: {config['ambient']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56ff16f",
   "metadata": {},
   "source": [
    "## 3. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18cab4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5000 training CY points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pointgen:INFO:Vol_k: 5.0, Vol_cy: 280.4873248050895.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training data generated:\n",
      "  Points in R^10: (5000, 10)\n",
      "  Points in C^5: (5000, 5)\n",
      "  Points in C^3 (local coords): (5000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Generate training points\n",
    "n_train = 5000  # Number of training points\n",
    "\n",
    "print(f\"Generating {n_train} training CY points...\")\n",
    "\n",
    "# Initialize point generator\n",
    "pg = PointGenerator(\n",
    "    config['monomials'], \n",
    "    config['coefficients'], \n",
    "    config['kmoduli'], \n",
    "    config['ambient']\n",
    ")\n",
    "\n",
    "# Generate dataset\n",
    "kappa = pg.prepare_dataset(n_train, dirname, val_split=0.)\n",
    "pg.prepare_basis(dirname, kappa=kappa)\n",
    "\n",
    "# Load generated data\n",
    "data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "points_train = data['X_train']\n",
    "\n",
    "# Convert to C^5 complex coordinates\n",
    "cy_points_train = CoordChange_C5R10(points_train, inverse=True)\n",
    "\n",
    "# Identify patch indices\n",
    "one_idxs_train = np.argmax(np.isclose(cy_points_train, complex(1, 0)), axis=1)\n",
    "dropped_idxs_train = pg._find_max_dQ_coords(cy_points_train)\n",
    "\n",
    "# Extract C^3 coordinates (excluding one_idx and dropped_idx)\n",
    "mask_train = np.ones(cy_points_train.shape, dtype=bool)\n",
    "samples_train = np.arange(cy_points_train.shape[0])\n",
    "mask_train[samples_train, one_idxs_train] = False\n",
    "mask_train[samples_train, dropped_idxs_train] = False\n",
    "c3_coords_train = cy_points_train[mask_train].reshape(cy_points_train.shape[0], -1)\n",
    "\n",
    "print(f\"âœ… Training data generated:\")\n",
    "print(f\"  Points in R^10: {points_train.shape}\")\n",
    "print(f\"  Points in C^5: {cy_points_train.shape}\")\n",
    "print(f\"  Points in C^3 (local coords): {c3_coords_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c73bca",
   "metadata": {},
   "source": [
    "## 4. Compute Holomorphic Volume Form and Hermitian Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute holomorphic volume form for training data\n",
    "holomorphic_volume_form_train = pg.holomorphic_volume_form(cy_points_train)\n",
    "\n",
    "print(f\"Holomorphic volume form (training):\")\n",
    "print(f\"  Shape: {holomorphic_volume_form_train.shape}\")\n",
    "print(f\"  Dtype: {holomorphic_volume_form_train.dtype}\")\n",
    "print(f\"  This is a complex (3,0)-form on the CY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98374938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load basis and create CyMetric model\n",
    "BASIS = np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True)\n",
    "BASIS_tf = prepare_tf_basis(BASIS)\n",
    "\n",
    "# Build neural network architecture\n",
    "nn_phi = tf.keras.Sequential()\n",
    "nn_phi.add(tf.keras.Input(shape=(config['n_in'],)))\n",
    "for _ in range(config['nlayer']):\n",
    "    nn_phi.add(tf.keras.layers.Dense(config['nHidden'], activation=config['act']))\n",
    "nn_phi.add(tf.keras.layers.Dense(config['n_out'], use_bias=False))\n",
    "\n",
    "# Create PhiFSModel\n",
    "cymetric_model = PhiFSModel(nn_phi, BASIS_tf, alpha=config['alpha'])\n",
    "cymetric_model.nn_phi = tf.keras.models.load_model(cymodel_path)\n",
    "\n",
    "# Compute Hermitian metric\n",
    "hermitian_metric_train = cymetric_model(CoordChange_C5R10(cy_points_train)).numpy()\n",
    "hermitian_metric_train = 0.5 * (hermitian_metric_train + hermitian_metric_train.conj().transpose(0, 2, 1))\n",
    "\n",
    "print(f\"âœ… CyMetric model loaded\")\n",
    "print(f\"Hermitian metric (training):\")\n",
    "print(f\"  Shape: {hermitian_metric_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9b716",
   "metadata": {},
   "source": [
    "## 5. Prepare Neural Network Training Data\n",
    "\n",
    "Extract the complex components of the holomorphic volume form and convert to real components for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The holomorphic volume form has only one independent component: Omega_{012}\n",
    "# Extract it\n",
    "print(f\"Omega_{{012}} components:\")\n",
    "print(f\"  Shape: {holomorphic_volume_form_train.shape}\")\n",
    "print(f\"  Sample values: {holomorphic_volume_form_train[:3]}\")\n",
    "\n",
    "# Convert to real and imaginary parts for training targets\n",
    "omega_real_train = np.real(holomorphic_volume_form_train)\n",
    "omega_imag_train = np.imag(holomorphic_volume_form_train)\n",
    "\n",
    "# Stack as [real, imag] for each point\n",
    "y_train = np.stack([omega_real_train, omega_imag_train], axis=1)\n",
    "\n",
    "print(f\"\\nTraining targets (real and imag parts):\")\n",
    "print(f\"  Shape: {y_train.shape}\")\n",
    "print(f\"  [real, imag] for first 3 points:\")\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecd8a1",
   "metadata": {},
   "source": [
    "## 6. Prepare Input Features\n",
    "\n",
    "Choose between:\n",
    "- **Option A:** R^10 coordinates (embedding space) from `points_train`\n",
    "- **Option B:** R^16 coordinates: C^3 local coordinates (as R^6) + one-hot encoded patch indices (10 dims: 5 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose input type\n",
    "INPUT_TYPE = 'c3'  # Options: 'r10' or 'c3'\n",
    "\n",
    "if INPUT_TYPE == 'r10':\n",
    "    # Use R^10 embedding coordinates\n",
    "    X_train = points_train\n",
    "    input_dim = 10\n",
    "    print(f\"Using R^10 embedding coordinates\")\n",
    "elif INPUT_TYPE == 'c3':\n",
    "    # Use C^3 local coordinates as R^6 (real and imaginary parts) + one-hot encoded patch indices\n",
    "    # One-hot encode patch indices (5 classes each)\n",
    "    one_idx_onehot_train = tf.keras.utils.to_categorical(one_idxs_train, num_classes=5)\n",
    "    dropped_idx_onehot_train = tf.keras.utils.to_categorical(dropped_idxs_train, num_classes=5)\n",
    "    \n",
    "    X_train = np.concatenate([\n",
    "        np.real(c3_coords_train), \n",
    "        np.imag(c3_coords_train),\n",
    "        one_idx_onehot_train,\n",
    "        dropped_idx_onehot_train\n",
    "    ], axis=1)\n",
    "    input_dim = 16  # 6 (coords) + 5 (one_idx) + 5 (dropped_idx)\n",
    "    print(f\"Using C^3 local coordinates (as R^6) + one-hot encoded patch indices (10)\")\n",
    "    print(f\"  Components: [Re(z1-3), Im(z1-3), one_hot(one_idx), one_hot(dropped_idx)]\")\n",
    "    print(f\"  Input dimension: 6 coords + 5 one_idx + 5 dropped_idx = 16\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown INPUT_TYPE: {INPUT_TYPE}\")\n",
    "\n",
    "print(f\"\\nInput features:\")\n",
    "print(f\"  Shape: {X_train.shape}\")\n",
    "print(f\"  Input dimension: {input_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9fa851",
   "metadata": {},
   "source": [
    "## 7. Generate Test Data (Independent Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f0f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test points\n",
    "n_test = 20000\n",
    "\n",
    "print(f\"Generating {n_test} test CY points...\")\n",
    "\n",
    "# Generate new dataset for testing\n",
    "kappa_test = pg.prepare_dataset(n_test, dirname, val_split=0.)\n",
    "data_test = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "points_test = data_test['X_train']\n",
    "\n",
    "# Convert to C^5 complex coordinates\n",
    "cy_points_test = CoordChange_C5R10(points_test, inverse=True)\n",
    "\n",
    "# Identify patch indices\n",
    "one_idxs_test = np.argmax(np.isclose(cy_points_test, complex(1, 0)), axis=1)\n",
    "dropped_idxs_test = pg._find_max_dQ_coords(cy_points_test)\n",
    "\n",
    "# Extract C^3 coordinates\n",
    "mask_test = np.ones(cy_points_test.shape, dtype=bool)\n",
    "samples_test = np.arange(cy_points_test.shape[0])\n",
    "mask_test[samples_test, one_idxs_test] = False\n",
    "mask_test[samples_test, dropped_idxs_test] = False\n",
    "c3_coords_test = cy_points_test[mask_test].reshape(cy_points_test.shape[0], -1)\n",
    "\n",
    "# Compute holomorphic volume form for test data\n",
    "holomorphic_volume_form_test = pg.holomorphic_volume_form(cy_points_test)\n",
    "omega_real_test = np.real(holomorphic_volume_form_test)\n",
    "omega_imag_test = np.imag(holomorphic_volume_form_test)\n",
    "y_test = np.stack([omega_real_test, omega_imag_test], axis=1)\n",
    "\n",
    "# Prepare test input features\n",
    "if INPUT_TYPE == 'r10':\n",
    "    X_test = points_test\n",
    "elif INPUT_TYPE == 'c3':\n",
    "    # One-hot encode test patch indices\n",
    "    one_idx_onehot_test = tf.keras.utils.to_categorical(one_idxs_test, num_classes=5)\n",
    "    dropped_idx_onehot_test = tf.keras.utils.to_categorical(dropped_idxs_test, num_classes=5)\n",
    "    \n",
    "    X_test = np.concatenate([\n",
    "        np.real(c3_coords_test), \n",
    "        np.imag(c3_coords_test),\n",
    "        one_idx_onehot_test,\n",
    "        dropped_idx_onehot_test\n",
    "    ], axis=1)\n",
    "\n",
    "print(f\"âœ… Test data generated:\")\n",
    "print(f\"  X_test shape: {X_test.shape}\")\n",
    "print(f\"  y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23e347d",
   "metadata": {},
   "source": [
    "## 8. Build and Train Neural Network\n",
    "\n",
    "### Hyperparameters (Edit these!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001afb6",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Suggestions for Improving Learning Performance\n",
    "\n",
    "If the model is performing poorly, try these approaches:\n",
    "\n",
    "### **1. Input Normalization/Standardization**\n",
    "- **Problem**: Input features may have very different scales\n",
    "- **Solution**: Add standardization of inputs (z-score normalization)\n",
    "- **Implementation**: Use `StandardScaler` or normalize manually: `X_norm = (X - X.mean()) / X.std()`\n",
    "- **Expected Impact**: HIGH - Often critical for neural networks\n",
    "\n",
    "### **2. Output Normalization**\n",
    "- **Problem**: Target values may have extreme ranges or be very small/large\n",
    "- **Solution**: Normalize outputs to [0,1] or standardize them\n",
    "- **Implementation**: Scale y_train before training, inverse transform predictions\n",
    "- **Expected Impact**: HIGH - Especially if target values are very small\n",
    "\n",
    "### **3. Network Architecture**\n",
    "- **Problem**: Network may be too small/large or wrong shape\n",
    "- **Options**:\n",
    "  - Increase depth: `[256, 256, 128, 64, 32]` (more layers for complex functions)\n",
    "  - Increase width: `[512, 256, 128]` (wider layers)\n",
    "  - Add residual connections (requires custom implementation)\n",
    "- **Expected Impact**: MEDIUM - Depends on problem complexity\n",
    "\n",
    "### **4. Activation Functions**\n",
    "- **Problem**: Current activation (gelu) may not suit this problem\n",
    "- **Options**:\n",
    "  - `'tanh'` - Good for bounded outputs, smooth gradients\n",
    "  - `'elu'` - Helps with vanishing gradients\n",
    "  - `'swish'` - Modern alternative to ReLU\n",
    "  - `'selu'` - Self-normalizing, but requires specific initialization\n",
    "- **Expected Impact**: LOW-MEDIUM\n",
    "\n",
    "### **5. Learning Rate & Optimizer**\n",
    "- **Problem**: LR may be too high/low, or optimizer suboptimal\n",
    "- **Options**:\n",
    "  - Lower LR: `0.0001` or `0.00001`\n",
    "  - Higher LR with warmup: Start at `0.0001`, increase to `0.001`\n",
    "  - Try different optimizer: `AdamW` (better weight decay)\n",
    "  - Use cosine annealing: `CosineAnnealingScheduler`\n",
    "- **Expected Impact**: MEDIUM-HIGH\n",
    "\n",
    "### **6. Batch Normalization**\n",
    "- **Problem**: Internal covariate shift slowing learning\n",
    "- **Solution**: Set `USE_BATCH_NORM = True`\n",
    "- **Expected Impact**: MEDIUM - Helps with training stability\n",
    "\n",
    "### **7. Regularization Tuning**\n",
    "- **Problem**: Underfitting (too much reg) or overfitting (too little)\n",
    "- **Options**:\n",
    "  - Reduce L2: Try `0.0001` or `0.00001` or `0.0`\n",
    "  - Reduce Dropout: `0.0` or `0.001` (current 0.01 may be too high)\n",
    "  - Try L1 regularization for sparsity\n",
    "- **Expected Impact**: MEDIUM\n",
    "\n",
    "### **8. More Training Data**\n",
    "- **Problem**: 50k samples may not be enough\n",
    "- **Solution**: Increase `n_train` to `100000` or `200000`\n",
    "- **Expected Impact**: MEDIUM-HIGH - More data usually helps\n",
    "\n",
    "### **9. Data Augmentation**\n",
    "- **Problem**: Limited diversity in training samples\n",
    "- **Solution**: Add noise to inputs during training, or generate more diverse points\n",
    "- **Expected Impact**: MEDIUM\n",
    "\n",
    "### **10. Loss Function**\n",
    "- **Problem**: MSE may not be optimal for this problem\n",
    "- **Options**:\n",
    "  - Try `'mae'` (less sensitive to outliers)\n",
    "  - Try `'huber'` (robust to outliers)\n",
    "  - Custom loss: Log-cosh, or weighted MSE\n",
    "- **Expected Impact**: LOW-MEDIUM\n",
    "\n",
    "### **11. Complex-Valued Network**\n",
    "- **Problem**: Learning real/imag separately loses phase information\n",
    "- **Solution**: Use complex-valued neural network layers (requires custom implementation)\n",
    "- **Expected Impact**: HIGH - But requires significant code changes\n",
    "\n",
    "### **12. Physics-Informed Constraints**\n",
    "- **Problem**: Network doesn't respect geometric properties\n",
    "- **Solution**: Add loss terms for known constraints (e.g., |Omega|Â² should match expectations)\n",
    "- **Expected Impact**: HIGH - Encodes domain knowledge\n",
    "\n",
    "### **Recommended Priority Order:**\n",
    "1. **START HERE**: Input/Output normalization (#1, #2) â­â­â­\n",
    "2. Lower learning rate and remove/reduce dropout (#5, #7)\n",
    "3. Increase training data (#8)\n",
    "4. Try different architecture (#3)\n",
    "5. Enable batch normalization (#6)\n",
    "6. Experiment with loss functions and activation (#10, #4)\n",
    "\n",
    "### **Quick Win Combination:**\n",
    "```python\n",
    "# Normalize inputs and outputs\n",
    "X_train_mean, X_train_std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train_norm = (X_train - X_train_mean) / (X_train_std + 1e-8)\n",
    "X_test_norm = (X_test - X_train_mean) / (X_train_std + 1e-8)\n",
    "\n",
    "y_train_mean, y_train_std = y_train.mean(axis=0), y_train.std(axis=0)\n",
    "y_train_norm = (y_train - y_train_mean) / (y_train_std + 1e-8)\n",
    "# Train on normalized data, then: y_pred_real = y_pred_norm * y_train_std + y_train_mean\n",
    "\n",
    "# Adjust hyperparameters\n",
    "HIDDEN_LAYERS = [256, 256, 128, 64]\n",
    "LEARNING_RATE = 0.0001\n",
    "DROPOUT_RATE = 0.0\n",
    "USE_BATCH_NORM = True\n",
    "L2_REG = 0.0001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ecbc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HYPERPARAMETERS - Edit these to tune the model\n",
    "# ============================================================================\n",
    "\n",
    "# Data preprocessing\n",
    "NORMALIZE_INPUTS = True        # Standardize input features (z-score normalization)\n",
    "NORMALIZE_OUTPUTS = True       # Standardize output targets (z-score normalization)\n",
    "\n",
    "# Architecture\n",
    "HIDDEN_LAYERS = [256, 256, 128, 64]  # List of hidden layer sizes\n",
    "ACTIVATION = 'gelu'            # Activation function: 'relu', 'gelu', 'tanh', 'elu'\n",
    "OUTPUT_ACTIVATION = None       # Output activation (None for linear)\n",
    "USE_BATCH_NORM = True         # Use batch normalization between layers\n",
    "DROPOUT_RATE = 0.01             # Dropout rate (0.0 = no dropout)\n",
    "\n",
    "# Training\n",
    "LEARNING_RATE = 0.0002          # Initial learning rate\n",
    "BATCH_SIZE = 128                # Batch size for training\n",
    "EPOCHS = 200                   # Maximum number of epochs\n",
    "VALIDATION_SPLIT = 0.1         # Fraction of training data for validation\n",
    "\n",
    "# Regularization\n",
    "L2_REG = 0.001                # L2 regularization coefficient (for layers)\n",
    "WEIGHT_DECAY = 0.001            # Weight decay for AdamW optimizer\n",
    "\n",
    "# Callbacks\n",
    "EARLY_STOPPING_PATIENCE = 100   # Stop if no improvement for this many epochs\n",
    "REDUCE_LR_PATIENCE = 15        # Reduce LR if no improvement for this many epochs\n",
    "REDUCE_LR_FACTOR = 0.5         # Factor to reduce LR by\n",
    "\n",
    "# Loss function\n",
    "LOSS = 'huber'                   # 'mse', 'mae', 'huber'\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Hyperparameters set:\")\n",
    "print(f\"  Normalize inputs: {NORMALIZE_INPUTS}\")\n",
    "print(f\"  Normalize outputs: {NORMALIZE_OUTPUTS}\")\n",
    "print(f\"  Architecture: {input_dim} -> {' -> '.join(map(str, HIDDEN_LAYERS))} -> 2\")\n",
    "print(f\"  Activation: {ACTIVATION}\")\n",
    "print(f\"  Batch normalization: {USE_BATCH_NORM}\")\n",
    "print(f\"  Dropout: {DROPOUT_RATE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max epochs: {EPOCHS}\")\n",
    "print(f\"  Loss: {LOSS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1fffbd",
   "metadata": {},
   "source": [
    "### Normalize Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize inputs and outputs based on flags\n",
    "if NORMALIZE_INPUTS:\n",
    "    if INPUT_TYPE == 'r10':\n",
    "        # R^10: Normalize all features\n",
    "        X_train_mean = X_train.mean(axis=0)\n",
    "        X_train_std = X_train.std(axis=0) + 1e-8\n",
    "        \n",
    "        X_train_normalized = (X_train - X_train_mean) / X_train_std\n",
    "        X_test_normalized = (X_test - X_train_mean) / X_train_std\n",
    "        \n",
    "        print(\"âœ… Input normalization applied (R^10):\")\n",
    "        print(f\"  All 10 dimensions normalized\")\n",
    "        \n",
    "    elif INPUT_TYPE == 'c3':\n",
    "        # C^3: Only normalize continuous coordinate features (first 6 dims)\n",
    "        # Keep one-hot encoded patch indices sparse (last 10 dims)\n",
    "        X_train_coords = X_train[:, :6]  # C^3 real and imaginary parts\n",
    "        X_train_patches = X_train[:, 6:]  # One-hot patches (keep sparse)\n",
    "        \n",
    "        X_test_coords = X_test[:, :6]\n",
    "        X_test_patches = X_test[:, 6:]\n",
    "        \n",
    "        # Normalize only coordinates\n",
    "        X_coords_mean = X_train_coords.mean(axis=0)\n",
    "        X_coords_std = X_train_coords.std(axis=0) + 1e-8\n",
    "        \n",
    "        X_train_coords_norm = (X_train_coords - X_coords_mean) / X_coords_std\n",
    "        X_test_coords_norm = (X_test_coords - X_coords_mean) / X_coords_std\n",
    "        \n",
    "        # Recombine: normalized coords + unnormalized sparse one-hot patches\n",
    "        X_train_normalized = np.concatenate([X_train_coords_norm, X_train_patches], axis=1)\n",
    "        X_test_normalized = np.concatenate([X_test_coords_norm, X_test_patches], axis=1)\n",
    "        \n",
    "        print(\"âœ… Input normalization applied (C^3 + patches):\")\n",
    "        print(f\"  Coordinates (dims 0-5) normalized: meanâ‰ˆ0, stdâ‰ˆ1\")\n",
    "        print(f\"  Patch indices (dims 6-15) kept sparse: values in {{0, 1}}\")\n",
    "        print(f\"  Normalized coords - mean: {X_train_coords_norm.mean(axis=0)[:3]}\")\n",
    "        print(f\"  Sparse patches - sample: {X_train_patches[0]}\")\n",
    "        \n",
    "    print(f\"  Original X_train shape: {X_train.shape}\")\n",
    "    print(f\"  Normalized X_train shape: {X_train_normalized.shape}\")\n",
    "else:\n",
    "    X_train_normalized = X_train\n",
    "    X_test_normalized = X_test\n",
    "    print(\"â„¹ï¸  Input normalization: DISABLED\")\n",
    "\n",
    "if NORMALIZE_OUTPUTS:\n",
    "    # Compute normalization statistics from training targets\n",
    "    y_train_mean = y_train.mean(axis=0)\n",
    "    y_train_std = y_train.std(axis=0) + 1e-8\n",
    "    \n",
    "    # Normalize training and test outputs\n",
    "    y_train_normalized = (y_train - y_train_mean) / y_train_std\n",
    "    y_test_normalized = (y_test - y_train_mean) / y_train_std\n",
    "    \n",
    "    print(\"\\nâœ… Output normalization applied:\")\n",
    "    print(f\"  Original y_train - mean: {y_train.mean(axis=0)}, std: {y_train.std(axis=0)}\")\n",
    "    print(f\"  Normalized y_train - mean: {y_train_normalized.mean(axis=0)}, std: {y_train_normalized.std(axis=0)}\")\n",
    "else:\n",
    "    y_train_normalized = y_train\n",
    "    y_test_normalized = y_test\n",
    "    y_train_mean = np.zeros(2)\n",
    "    y_train_std = np.ones(2)\n",
    "    print(\"\\nâ„¹ï¸  Output normalization: DISABLED\")\n",
    "\n",
    "# Use normalized data for training\n",
    "X_train_final = X_train_normalized\n",
    "X_test_final = X_test_normalized\n",
    "y_train_final = y_train_normalized\n",
    "y_test_final = y_test_normalized\n",
    "\n",
    "print(f\"\\nðŸ“Š Final training data shapes:\")\n",
    "print(f\"  X_train_final: {X_train_final.shape}\")\n",
    "print(f\"  y_train_final: {y_train_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8b610",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcaeaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(input_dim,)))\n",
    "\n",
    "# Add hidden layers\n",
    "for i, hidden_size in enumerate(HIDDEN_LAYERS):\n",
    "    if L2_REG > 0:\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            hidden_size, \n",
    "            activation=ACTIVATION,\n",
    "            kernel_regularizer=tf.keras.regularizers.L2(L2_REG)\n",
    "        ))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(hidden_size, activation=ACTIVATION))\n",
    "    \n",
    "    if USE_BATCH_NORM:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    if DROPOUT_RATE > 0:\n",
    "        model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "\n",
    "# Output layer (2 outputs: real and imaginary parts)\n",
    "model.add(tf.keras.layers.Dense(2, activation=OUTPUT_ACTIVATION))\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b2d27e",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with AdamW optimizer\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    ),\n",
    "    loss=LOSS,\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "print(\"âœ… Model compiled\")\n",
    "print(f\"  Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")\n",
    "print(f\"  Loss: {LOSS}\")\n",
    "print(f\"  Metrics: MAE, MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e91018",
   "metadata": {},
   "source": [
    "### Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c631bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "callbacks = []\n",
    "\n",
    "# Early stopping\n",
    "if EARLY_STOPPING_PATIENCE > 0:\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(early_stopping)\n",
    "    print(f\"Early stopping enabled (patience={EARLY_STOPPING_PATIENCE})\")\n",
    "\n",
    "# Learning rate reduction\n",
    "if REDUCE_LR_PATIENCE > 0:\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=REDUCE_LR_FACTOR,\n",
    "        patience=REDUCE_LR_PATIENCE,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(reduce_lr)\n",
    "    print(f\"LR reduction enabled (patience={REDUCE_LR_PATIENCE}, factor={REDUCE_LR_FACTOR})\")\n",
    "\n",
    "print(f\"\\nTotal callbacks: {len(callbacks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab1962",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c0af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(f\"\\nTraining model on {len(X_train_final)} samples...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final,\n",
    "    y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de3945",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc7c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "\n",
    "test_results = model.evaluate(X_test_final, y_test_final, verbose=0)\n",
    "\n",
    "print(f\"Test Results (on normalized data):\")\n",
    "print(f\"  Loss ({LOSS}): {test_results[0]:.6f}\")\n",
    "print(f\"  MAE: {test_results[1]:.6f}\")\n",
    "print(f\"  MSE: {test_results[2]:.6f}\")\n",
    "\n",
    "# Make predictions on normalized inputs\n",
    "y_pred_normalized = model.predict(X_test_final, verbose=0)\n",
    "\n",
    "# Denormalize predictions and targets for metric calculation\n",
    "if NORMALIZE_OUTPUTS:\n",
    "    y_pred = y_pred_normalized * y_train_std + y_train_mean\n",
    "    y_test_denorm = y_test  # Original unnormalized test data\n",
    "    print(\"\\nâœ… Predictions denormalized for final metrics\")\n",
    "else:\n",
    "    y_pred = y_pred_normalized\n",
    "    y_test_denorm = y_test\n",
    "\n",
    "# Calculate additional metrics on original scale\n",
    "real_mae = np.mean(np.abs(y_test_denorm[:, 0] - y_pred[:, 0]))\n",
    "imag_mae = np.mean(np.abs(y_test_denorm[:, 1] - y_pred[:, 1]))\n",
    "real_mse = np.mean((y_test_denorm[:, 0] - y_pred[:, 0])**2)\n",
    "imag_mse = np.mean((y_test_denorm[:, 1] - y_pred[:, 1])**2)\n",
    "\n",
    "print(f\"\\nPer-component metrics (original scale):\")\n",
    "print(f\"  Real part - MAE: {real_mae:.6f}, MSE: {real_mse:.6f}\")\n",
    "print(f\"  Imag part - MAE: {imag_mae:.6f}, MSE: {imag_mse:.6f}\")\n",
    "\n",
    "# MAPE relative to mean of absolute values (since true mean is ~0)\n",
    "real_mean_abs = np.mean(np.abs(y_test_denorm[:, 0]))\n",
    "imag_mean_abs = np.mean(np.abs(y_test_denorm[:, 1]))\n",
    "\n",
    "real_mape_rel = 100 * real_mae / real_mean_abs\n",
    "imag_mape_rel = 100 * imag_mae / imag_mean_abs\n",
    "\n",
    "print(f\"\\nMAPE relative to mean(|target|):\")\n",
    "print(f\"  Real part - mean(|target|): {real_mean_abs:.6f}, MAPE: {real_mape_rel:.2f}%\")\n",
    "print(f\"  Imag part - mean(|target|): {imag_mean_abs:.6f}, MAPE: {imag_mape_rel:.2f}%\")\n",
    "\n",
    "# Standard MAPE (only for non-zero targets)\n",
    "real_nonzero_mask = np.abs(y_test_denorm[:, 0]) > 1e-10\n",
    "imag_nonzero_mask = np.abs(y_test_denorm[:, 1]) > 1e-10\n",
    "\n",
    "if np.sum(real_nonzero_mask) > 0:\n",
    "    real_mape = 100 * np.mean(np.abs((y_test_denorm[real_nonzero_mask, 0] - y_pred[real_nonzero_mask, 0]) / y_test_denorm[real_nonzero_mask, 0]))\n",
    "    print(f\"\\nStandard MAPE (element-wise, non-zero only):\")\n",
    "    print(f\"  Real part - MAPE: {real_mape:.2f}% ({np.sum(real_nonzero_mask)}/{len(y_test)} non-zero)\")\n",
    "\n",
    "if np.sum(imag_nonzero_mask) > 0:\n",
    "    imag_mape = 100 * np.mean(np.abs((y_test_denorm[imag_nonzero_mask, 1] - y_pred[imag_nonzero_mask, 1]) / y_test_denorm[imag_nonzero_mask, 1]))\n",
    "    print(f\"  Imag part - MAPE: {imag_mape:.2f}% ({np.sum(imag_nonzero_mask)}/{len(y_test)} non-zero)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fdbe0e",
   "metadata": {},
   "source": [
    "## 10. Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b312ac",
   "metadata": {},
   "source": [
    "### Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1].set_title('Training and Validation MAE', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595017ff",
   "metadata": {},
   "source": [
    "### Prediction vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2111073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots: predicted vs actual (on original scale)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Calculate Pearson correlation coefficients\n",
    "real_corr = np.corrcoef(y_test_denorm[:, 0], y_pred[:, 0])[0, 1]\n",
    "imag_corr = np.corrcoef(y_test_denorm[:, 1], y_pred[:, 1])[0, 1]\n",
    "\n",
    "# Real part\n",
    "axes[0].scatter(y_test_denorm[:, 0], y_pred[:, 0], alpha=0.5, s=20)\n",
    "min_real = min(y_test_denorm[:, 0].min(), y_pred[:, 0].min())\n",
    "max_real = max(y_test_denorm[:, 0].max(), y_pred[:, 0].max())\n",
    "axes[0].plot([min_real, max_real], [min_real, max_real], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual (Real Part)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted (Real Part)', fontsize=12)\n",
    "axes[0].set_title(f'Real Part: Predicted vs Actual\\nMAE={real_mae:.6f}, PMCC={real_corr:.4f}', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axis('equal')\n",
    "\n",
    "# Imaginary part\n",
    "axes[1].scatter(y_test_denorm[:, 1], y_pred[:, 1], alpha=0.5, s=20, color='orange')\n",
    "min_imag = min(y_test_denorm[:, 1].min(), y_pred[:, 1].min())\n",
    "max_imag = max(y_test_denorm[:, 1].max(), y_pred[:, 1].max())\n",
    "axes[1].plot([min_imag, max_imag], [min_imag, max_imag], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual (Imag Part)', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted (Imag Part)', fontsize=12)\n",
    "axes[1].set_title(f'Imaginary Part: Predicted vs Actual\\nMAE={imag_mae:.6f}, PMCC={imag_corr:.4f}', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d564bf2",
   "metadata": {},
   "source": [
    "### Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15410aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error distributions (on original scale)\n",
    "real_errors = y_test_denorm[:, 0] - y_pred[:, 0]\n",
    "imag_errors = y_test_denorm[:, 1] - y_pred[:, 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Real part errors\n",
    "axes[0].hist(real_errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[0].set_xlabel('Error (Actual - Predicted)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title(f'Real Part Error Distribution\\nMean={np.mean(real_errors):.6f}, Std={np.std(real_errors):.6f}', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Imaginary part errors\n",
    "axes[1].hist(imag_errors, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1].axvline(0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[1].set_xlabel('Error (Actual - Predicted)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title(f'Imaginary Part Error Distribution\\nMean={np.mean(imag_errors):.6f}, Std={np.std(imag_errors):.6f}', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502df6f5",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Input type: {INPUT_TYPE}\")\n",
    "print(f\"  Input dimension: {input_dim}\")\n",
    "print(f\"  Output dimension: 2 (real + imag)\")\n",
    "\n",
    "print(f\"\\nModel:\")\n",
    "print(f\"  Architecture: {input_dim} -> {' -> '.join(map(str, HIDDEN_LAYERS))} -> 2\")\n",
    "print(f\"  Total parameters: {model.count_params():,}\")\n",
    "print(f\"  Activation: {ACTIVATION}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs run: {len(history.history['loss'])}\")\n",
    "print(f\"  Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "print(f\"  Final validation loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  Overall MAE: {test_results[1]:.6f}\")\n",
    "print(f\"  Real part MAE: {real_mae:.6f}\")\n",
    "print(f\"  Imag part MAE: {imag_mae:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "G2 ML Environment",
   "language": "python",
   "name": "g2_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
